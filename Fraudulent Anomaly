# Import necessary libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import pickle
import shap  # For explainability
from sklearn.metrics import accuracy_score

# Load the dataset
credit_df = pd.read_csv('/content/User0_credit_card_transactions.csv')

# Fill missing values using forward fill
credit_df.fillna(method='ffill', inplace=True)

# Remove the '$' sign from 'Amount' and convert it to numeric
credit_df['Amount'] = credit_df['Amount'].replace({'\$': '', ',': ''}, regex=True).astype(float)

# Select relevant features for anomaly detection
# If there are any other numeric columns, include them here
features = credit_df[['Amount']].copy()

# Normalize the 'Amount' column
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Train the Isolation Forest model
iso_forest = IsolationForest(contamination=0.05, random_state=42)
credit_df['anomaly_score'] = iso_forest.fit_predict(features_scaled)

# Flag anomalies (-1 is an anomaly, 1 is normal)
credit_df['anomaly'] = credit_df['anomaly_score'].apply(lambda x: 'Anomaly' if x == -1 else 'Normal')

# Calculate and print the number of anomalies
num_anomalies = len(credit_df[credit_df['anomaly'] == 'Anomaly'])
print(f"Number of anomalies detected: {num_anomalies}")

# Summarize background data using k-means
background_data = shap.kmeans(features_scaled, 10)  # Use 100 clusters for summarization

# Compute SHAP values for feature contributions
# Using Isolation Forest with SHAP (TreeExplainer)
#explainer = shap.TreeExplainer(iso_forest)  # Replaced with KernelExplainer
explainer = shap.KernelExplainer(iso_forest.decision_function, background_data) # Pass background data
shap_values = explainer.shap_values(features_scaled)

# Add SHAP explanations for anomalies
def get_explanation(index):
    # Format the SHAP values into a readable explanation
    shap_explanation = shap_values[index]
    explanation = {
        "feature_contributions": [
            {"feature": features.columns[i], "contribution": shap_explanation[i]}
            for i in range(len(features.columns))
        ]
    }

    # Add the explanation text
    explanation_text = f"This transaction increased the suspicion score by {shap_explanation[0]:.2f}, due to the feature '{features.columns[0]}'."
    explanation["explanation_text"] = explanation_text  # Add the text to the dictionary

    return explanation

# Add explanations to the anomalies
anomalies = credit_df[credit_df['anomaly'] == 'Anomaly']
anomalies['explanation'] = anomalies.index.map(lambda i: get_explanation(i))

# Display anomalies with explanations
print(anomalies[['User', 'Card', 'Year', 'Month', 'Day', 'Time', 'Amount', 'Merchant Name', 'Is Fraud?', 'explanation']])

# Save anomalies with explanations to a new JSON
anomalies.to_json('anomalies_with_explanations.json', orient = 'records')

# Visualize anomalies in the 'Amount' column
plt.scatter(credit_df.index, credit_df['Amount'],
            c=credit_df['anomaly'].apply(lambda x: 1 if x == 'Anomaly' else 0),
            cmap='coolwarm')
plt.title('Anomaly Detection in Credit Card Transactions')
plt.xlabel('Transaction Index')
plt.ylabel('Transaction Amount')
plt.show()

# Save the trained Isolation Forest model
with open('isolation_forest_model.pkl', 'wb') as f:
    pickle.dump(iso_forest, f)


