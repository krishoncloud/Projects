{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install Faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IieN2aBa0LzK",
        "outputId": "809ae2d6-ae91-40b2-c864-01a970aada0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Faker\n",
            "  Downloading Faker-36.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from Faker) (2025.1)\n",
            "Downloading Faker-36.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Faker\n",
            "Successfully installed Faker-36.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mng7tnAkyCg",
        "outputId": "a5f54f7d-3f73-4064-967e-e197d5ba1ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake dataset saved to 'fake_sign_language_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker for random word generation\n",
        "fake = Faker()\n",
        "\n",
        "# Define the number of rows and sensors\n",
        "num_rows = 100000\n",
        "num_sensors = 6  # Number of sensor values to generate\n",
        "\n",
        "# Generate random words and sensor values\n",
        "data = []\n",
        "for _ in range(num_rows):\n",
        "    # Use fake.word() to generate random words (not necessarily unique)\n",
        "    word = fake.word()\n",
        "    sensor_values = np.random.uniform(0, 1, num_sensors).tolist()  # Generate 6 random sensor values\n",
        "    data.append([word] + sensor_values)\n",
        "\n",
        "# Create a DataFrame\n",
        "columns = [\"word\"] + [f\"sensor_{i+1}\" for i in range(num_sensors)]\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"fake_sign_language_dataset.csv\", index=False)\n",
        "print(\"Fake dataset saved to 'fake_sign_language_dataset.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load the dataset\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Loads the dataset from a CSV file.\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file.\n",
        "    Returns:\n",
        "        X (np.array): Sensor data of shape (samples, 15).\n",
        "        y (np.array): Word labels of shape (samples,).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.iloc[:, 1:].values  # Sensor values (columns 1 to 15)\n",
        "    y = df.iloc[:, 0].values   # Word labels (column 0)\n",
        "    return X, y\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(X, y):\n",
        "    \"\"\"\n",
        "    Preprocesses the data by normalizing sensor values and encoding word labels.\n",
        "    Args:\n",
        "        X (np.array): Sensor data of shape (samples, 15).\n",
        "        y (np.array): Word labels of shape (samples,).\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test: Split and preprocessed data.\n",
        "    \"\"\"\n",
        "    # Normalize sensor values\n",
        "    scaler = StandardScaler()\n",
        "    X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "    # Encode word labels as integers\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_encoded, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test, label_encoder\n",
        "\n",
        "# Define the Gesture-to-Word Model\n",
        "def build_gesture_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Builds a neural network model to map sensor data to words.\n",
        "    Args:\n",
        "        input_shape (int): Number of input features (15 sensor values).\n",
        "        num_classes (int): Number of output classes (words).\n",
        "    Returns:\n",
        "        model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_shape,)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')  # Output layer for word classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the dataset\n",
        "    file_path = \"fake_sign_language_dataset.csv\"\n",
        "    X, y = load_dataset(file_path)\n",
        "\n",
        "    # Preprocess the data\n",
        "    X_train, X_test, y_train, y_test, label_encoder = preprocess_data(X, y)\n",
        "\n",
        "    # Build the model\n",
        "    num_classes = len(label_encoder.classes_)  # Number of unique words\n",
        "    gesture_model = build_gesture_model(input_shape=15, num_classes=num_classes)\n",
        "\n",
        "    # Train the model\n",
        "    gesture_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Save the model\n",
        "    gesture_model.save(\"gesture_to_word_model.h5\")\n",
        "    print(\"Gesture-to-Word Model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsxPHc7Rl8Tj",
        "outputId": "f9cbf39f-b5fd-4b7c-895b-2a2acd5662ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 7.5533e-04 - loss: 6.8818 - val_accuracy: 8.0000e-04 - val_loss: 6.8799\n",
            "Epoch 2/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.0011 - loss: 6.8748 - val_accuracy: 9.0000e-04 - val_loss: 6.8843\n",
            "Epoch 3/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.0019 - loss: 6.8605 - val_accuracy: 0.0011 - val_loss: 6.8922\n",
            "Epoch 4/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.0022 - loss: 6.8334 - val_accuracy: 7.0000e-04 - val_loss: 6.9192\n",
            "Epoch 5/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.0024 - loss: 6.8024 - val_accuracy: 9.5000e-04 - val_loss: 6.9443\n",
            "Epoch 6/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.0033 - loss: 6.7714 - val_accuracy: 9.5000e-04 - val_loss: 6.9619\n",
            "Epoch 7/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.0037 - loss: 6.7461 - val_accuracy: 9.0000e-04 - val_loss: 6.9873\n",
            "Epoch 8/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.0043 - loss: 6.7248 - val_accuracy: 9.5000e-04 - val_loss: 7.0191\n",
            "Epoch 9/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.0046 - loss: 6.7050 - val_accuracy: 6.5000e-04 - val_loss: 7.0411\n",
            "Epoch 10/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.0043 - loss: 6.6794 - val_accuracy: 0.0011 - val_loss: 7.0705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gesture-to-Word Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = gesture_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUFYx5kmeey",
        "outputId": "c8e1e002-f03c-450c-f029-f1a6d1ca8cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 6.8376e-04 - loss: 7.0647\n",
            "Test Accuracy: 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from faker import Faker\n",
        "\n",
        "# # Step 1: Generate Fake Dataset\n",
        "# def generate_fake_dataset():\n",
        "#     \"\"\"\n",
        "#     Generates a fake dataset with 100,000 words and 15 sensor values each.\n",
        "#     \"\"\"\n",
        "#     fake = Faker()\n",
        "#     num_rows = 100000\n",
        "#     num_sensors = 15\n",
        "#     data = []\n",
        "#     for word_id in range(num_rows):\n",
        "#         word = fake.unique.word()  # Generate a unique random word\n",
        "#         base_pattern = np.sin(np.linspace(0, 2 * np.pi * (word_id % 10), num_sensors))  # Unique pattern for each word\n",
        "#         noise = np.random.uniform(-0.1, 0.1, num_sensors)  # Add some noise\n",
        "#         sensor_values = base_pattern + noise  # Combine base pattern and noise\n",
        "#         data.append([word] + sensor_values.tolist())  # Add word and sensor values to the dataset\n",
        "#     columns = [\"word\"] + [f\"sensor_{i+1}\" for i in range(num_sensors)]  # Column names\n",
        "#     df = pd.DataFrame(data, columns=columns)  # Create a DataFrame\n",
        "#     df.to_csv(\"fake_sign_language_dataset.csv\", index=False)  # Save to CSV\n",
        "#     print(\"Fake dataset saved to 'fake_sign_language_dataset.csv'\")\n",
        "\n",
        "# Step 2: Train the Gesture-to-Word Model\n",
        "def train_gesture_to_word_model():\n",
        "    \"\"\"\n",
        "    Trains a neural network to map sensor data to words.\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(\"fake_sign_language_dataset.csv\")\n",
        "    X = df.iloc[:, 1:].values  # Sensor values (columns 1 to 15)\n",
        "    y = df.iloc[:, 0].values   # Word labels (column 0)\n",
        "\n",
        "    # Preprocess data\n",
        "    scaler = StandardScaler()\n",
        "    X_normalized = scaler.fit_transform(X)  # Normalize sensor values\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)  # Encode words as integers\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_encoded, test_size=0.2, random_state=42)  # Split data\n",
        "\n",
        "    # Build and train model\n",
        "    num_classes = len(label_encoder.classes_)  # Number of unique words\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(15,)),  # Input layer for 15 sensor values\n",
        "        layers.Dense(128, activation='relu'),  # Hidden layer 1\n",
        "        layers.Dropout(0.2),  # Dropout for regularization\n",
        "        layers.Dense(64, activation='relu'),  # Hidden layer 2\n",
        "        layers.Dense(num_classes, activation='softmax')  # Output layer for word classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Compile model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))  # Train model\n",
        "\n",
        "    # Save model and preprocessing objects\n",
        "    model.save(\"gesture_to_word_model.h5\")  # Save the trained model\n",
        "    np.save(\"label_encoder_classes.npy\", label_encoder.classes_)  # Save label encoder classes\n",
        "    np.save(\"scaler_mean.npy\", scaler.mean_)  # Save scaler mean\n",
        "    np.save(\"scaler_scale.npy\", scaler.scale_)  # Save scaler scale\n",
        "    print(\"Gesture-to-Word Model saved.\")\n",
        "\n",
        "# Step 3: Gesture-to-Sentence Pipeline\n",
        "def gesture_to_sentence():\n",
        "    \"\"\"\n",
        "    Captures a sequence of gestures and forms a sentence.\n",
        "    \"\"\"\n",
        "    # Load model and preprocessing objects\n",
        "    model = tf.keras.models.load_model(\"gesture_to_word_model.h5\")  # Load the trained model\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.classes_ = np.load(\"label_encoder_classes.npy\", allow_pickle=True)  # Load label encoder classes\n",
        "    scaler = StandardScaler()\n",
        "    scaler.mean_ = np.load(\"scaler_mean.npy\")  # Load scaler mean\n",
        "    scaler.scale_ = np.load(\"scaler_scale.npy\")  # Load scaler scale\n",
        "\n",
        "    # Function to predict a word\n",
        "    def predict_word(sensor_data):\n",
        "        \"\"\"\n",
        "        Predicts the word from sensor data.\n",
        "        \"\"\"\n",
        "        sensor_data_normalized = scaler.transform(sensor_data)  # Normalize sensor data\n",
        "        predicted_index = np.argmax(model.predict(sensor_data_normalized), axis=1)  # Predict word index\n",
        "        word = label_encoder.inverse_transform(predicted_index)[0]  # Convert index to word\n",
        "        return word\n",
        "\n",
        "    # Capture gestures and form a sentence\n",
        "    sentence = []  # List to store predicted words\n",
        "    print(\"Start making gestures. Type 'done' when finished.\")\n",
        "    while True:\n",
        "        user_input = input(\"Press Enter to capture a gesture or type 'done': \")  # Simulate user input\n",
        "        if user_input.lower() == \"done\":\n",
        "            break\n",
        "        sensor_data = np.random.uniform(0, 1, (1, 15))  # Simulate sensor data (replace with real data)\n",
        "        print(\"Sensor data captured:\", sensor_data)\n",
        "        word = predict_word(sensor_data)  # Predict the word\n",
        "        print(f\"Predicted word: {word}\")\n",
        "        sentence.append(word)  # Add the word to the sentence\n",
        "    final_sentence = \" \".join(sentence)  # Stitch words into a sentence\n",
        "    print(\"\\nFinal Sentence:\", final_sentence)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Generate fake dataset (run once)\n",
        "    # generate_fake_dataset()\n",
        "\n",
        "    # Step 2: Train the model (run once)\n",
        "    # train_gesture_to_word_model()\n",
        "\n",
        "    # Step 3: Run the Gesture-to-Sentence pipeline\n",
        "    gesture_to_sentence()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "lRkVRM_vrnSr",
        "outputId": "558af422-ebe2-47fa-a107-ddea54ce52c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'label_encoder_classes.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dbd840181b3f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Step 3: Run the Gesture-to-Sentence pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mgesture_to_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-dbd840181b3f>\u001b[0m in \u001b[0;36mgesture_to_sentence\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gesture_to_word_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label_encoder_classes.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load label encoder classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaler_mean.npy\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load scaler mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'label_encoder_classes.npy'"
          ]
        }
      ]
    }
  ]
}